{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Computational_Astrophysics.ipynb",
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyP7QdCUXFK7wYouWsWQki82",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/CuriousBrain2304/Computational_Astrophysics/blob/main/Computational_Astrophysics.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "thBg4hGZp-WX"
      },
      "source": [
        "## Calculate the Mean Stack"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ohum9rMWjIBC"
      },
      "source": [
        "def calculate_mean(data):\r\n",
        "  mean = sum(data)/len(data)\r\n",
        "  return mean"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9asdympgpbZE"
      },
      "source": [
        "data = [1.2,4.5,6.4,9.6]\r\n",
        "a = calculate_mean(data)\r\n",
        "a"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MvN0CSREpnm6"
      },
      "source": [
        "a = calculate_mean([1.2,4.5,6.4,9.6])\r\n",
        "a"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AyW5-ETxqc9v"
      },
      "source": [
        "NumPy arrays"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M09wpPmpqgot"
      },
      "source": [
        "import numpy as np"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dcSLYFmzq5BE"
      },
      "source": [
        "data = np.array([21.1,67.2,89.1,90.3])\r\n",
        "a = np.mean(data)\r\n",
        "b = np.size(data)\r\n",
        "b"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OLUuo7J0tg8J"
      },
      "source": [
        " Our file has several rows and columns. We want to store each row in a list and the whole file as a list of these rows.\r\n",
        "\r\n",
        "The program loops over each line in the file, splitting the row into a list of values, and appending each row to data: "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OrFxfIfqrX3L"
      },
      "source": [
        "data= []\r\n",
        "for line in open('data.csv'):\r\n",
        "  data.append(line.strip().split(','))\r\n",
        "\r\n",
        "print(data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7QdhOnqmtkUi"
      },
      "source": [
        "# Reading numbers from CSV files\r\n",
        "\r\n",
        "Now we can store the data in lists, we need to convert each item from a string to a float. We could do this using nested for loops: "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QcBxAIUzsPuJ"
      },
      "source": [
        "import numpy as np\r\n",
        "\r\n",
        "data = []\r\n",
        "for line in open('data.csv'):\r\n",
        "  data.append(line.strip().split(','))\r\n",
        "\r\n",
        "data = np.asarray(data, float)\r\n",
        "print(data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Q8vBMYmuEb3"
      },
      "source": [
        "# Reading a NumPy array from CSV \r\n",
        "The NumPy loadtxt function can automatically read a CSV file into a NumPy array, including converting from string to number"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QwyfWMQquL8x"
      },
      "source": [
        "import numpy as np\r\n",
        "data = np.loadtxt('data.csv', delimiter=',')\r\n",
        "print(data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dwyWWa0QuieJ"
      },
      "source": [
        "# NumPy arrays: element-wise operations\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ioIpvHwluqai"
      },
      "source": [
        "import numpy as np\r\n",
        "\r\n",
        "a = np.array([1, 2, 3])\r\n",
        "b = np.array([4, 5, 6])\r\n",
        "\r\n",
        "# Element-wise multiplication \r\n",
        "print(a*2)\r\n",
        "\r\n",
        "# Element-wise summation \r\n",
        "print(a + b)\r\n",
        "\r\n",
        "# Element-wise product \r\n",
        "print(a*b)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kMVA6w9XQcmQ"
      },
      "source": [
        "# Mean of a set of signals\r\n",
        " The files each contain n rows and m columns, giving a total of n x m cells. The individual cells are separated by commas, and all CSV files in the list will have the same number of rows and columns.\r\n",
        "\r\n",
        "The result should have the same dimensions as the input files. The result should be a NumPy array with individual entries rounded to one decimal place.\r\n",
        "\r\n",
        "Suppose we want to use the three files data1.csv, data2.csv and data3.csv. Your function should then take a list of the filenames and return the following: \r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ogj0oIOGQfPG"
      },
      "source": [
        "import numpy as np\r\n",
        "\r\n",
        "def mean_datasets(filenames):\r\n",
        "  n = len(filenames)\r\n",
        "  if n > 0:\r\n",
        "    data = np.loadtxt(filenames[0], delimiter=',')\r\n",
        "    for i in range(1,n):\r\n",
        "      data += np.loadtxt(filenames[i], delimiter=',')\r\n",
        "    \r\n",
        "    # Mean across all files:\r\n",
        "    data_mean = data/n\r\n",
        "     \r\n",
        "    return np.round(data_mean, 1)\r\n",
        "\r\n",
        "a  =  mean_datasets(['data1.csv', 'data2.csv', 'data3.csv'])\r\n",
        "print(a)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LWsJfMchQ2QO"
      },
      "source": [
        "# Working with FITS files\r\n",
        " One of the most widely used formats for astronomical images is the Flexible Image Transport System. In a FITS file, the image is stored in a numerical array, which we can load into a NumPy array.\r\n",
        "\r\n",
        "FITS files also have headers which store metadata about the image.\r\n",
        "\r\n",
        "FITS files are a standard format and astronomers have developed many libraries (in many programming languages) that can read and write FITS files. \r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R3N5MuH3uu9q"
      },
      "source": [
        "from astropy.io import fits\r\n",
        "hdulist = fits.open('image0.fits')\r\n",
        "hdulist.info()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "srmj9a-DTcid"
      },
      "source": [
        "# Reading in FITS file\r\n",
        " Opening a FITS file in Astropy returns a HDU (Header/Data Unit) list. Each HDU stores headers and (optionally) image data.\r\n",
        "\r\n",
        "The header contains metadata about the HDU object, e.g. its dimensions and data type. Every HDU can contain image data. The first HDU is called the primary HDU.\r\n",
        "\r\n",
        "If we want to access individual HDUs, we can index the HDU list object returned by fits.open. The image data can be accessed using the data attribute: \r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oDYv2Z2JS75M"
      },
      "source": [
        "from astropy.io import fits\r\n",
        "\r\n",
        "hdulist = fits.open('image0.fits')\r\n",
        "data = hdulist[0].data\r\n",
        "\r\n",
        "print(data.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m-Og87G0VMlo"
      },
      "source": [
        "# Visualising FITS image\r\n",
        "the image data stored in FITS files. We can do this using the plotting library matplotlib. \r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bgr6LB73VbYK"
      },
      "source": [
        "from astropy.io import fits\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "\r\n",
        "hdulist = fits.open('image0.fits')\r\n",
        "data = hdulist[0].data\r\n",
        "\r\n",
        "# Plot the 2D array\r\n",
        "plt.imshow(data, cmap=plt.cm.viridis)\r\n",
        "plt.xlabel('x-pixels (RA)')\r\n",
        "plt.ylabel('y-pixels (Dec)')\r\n",
        "plt.colorbar()\r\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hA5_aR4FVfiC"
      },
      "source": [
        "# Read a FITS file\r\n",
        "Write a load_fits function that loads in a FITS file and finds the position of the brightest pixel (i.e. the maximum value) in its image data. To make this function work for arbitrary files, pass the name of the FITS file as an argument to the function. \r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1XmM1-17V2o8"
      },
      "source": [
        "from astropy.io import fits\r\n",
        "import numpy as np\r\n",
        "\r\n",
        "def load_fits(filename):\r\n",
        "  hdulist = fits.open(filename)\r\n",
        "  data = hdulist[0].data\r\n",
        "\r\n",
        "  arg_max = np.argmax(data)\r\n",
        "  max_pos = np.unravel_index(arg_max , data.shape)\r\n",
        "\r\n",
        "  return max_pos"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zvroDmNveDx9"
      },
      "source": [
        "# Mean of a set of FITS file\r\n",
        "Write a mean_fits function that takes a list of FITS files as an argument, reads them in, and returns the mean image data of the FITS files. All the images have the same dimensions and your calculated mean array should match those dimensions. \r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sDklNROveOlJ"
      },
      "source": [
        "from astropy.io import \r\n",
        "import numpy as np\r\n",
        "\r\n",
        "def mean_fits(files):\r\n",
        "  n = len(files)\r\n",
        "  if n>0:\r\n",
        "    \r\n",
        "    hdulist = fits.open(files[0])\r\n",
        "    data = hdulist[0].data\r\n",
        "    hdulist.close()\r\n",
        "\r\n",
        "    for i in range (1,n):\r\n",
        "      hdulist = fits.open(files[i])\r\n",
        "      data += hdulist[0].data\r\n",
        "      hdulist.close()\r\n",
        "\r\n",
        "    mean = data / n\r\n",
        "    return mean\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fqSDpkFxEIrd"
      },
      "source": [
        "# Calculate the Median stack\r\n",
        "The median can be a more robust measure of the average trend of datasets than the mean, as the latter is easily skewed by outliers."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aunCD_4NETnk"
      },
      "source": [
        "import numpy as np\r\n",
        "\r\n",
        "def list_stats(values):\r\n",
        "    \r\n",
        "    N = len(values)\r\n",
        "    if N == 0:\r\n",
        "       return\r\n",
        "\r\n",
        "    # Mean\r\n",
        "    mean = sum(values)/N\r\n",
        "\r\n",
        "    # Median\r\n",
        "    values.sort()\r\n",
        "    mid = int(N/2)\r\n",
        "    if N%2 == 0:\r\n",
        "        median = (values[mid] + values[mid - 1])/2\r\n",
        "    else:\r\n",
        "        median = values[mid]\r\n",
        "\r\n",
        "    return median, mean  \r\n",
        "\r\n",
        "a = list_stats([1.3, 2.4, 20.6, 0.95, 3.1])\r\n",
        "print(a)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i7oaW7aFFKj0"
      },
      "source": [
        "import time\r\n",
        "start = time.perf_counter()\r\n",
        "# potentially slow computation\r\n",
        "end = time.perf_counter() - start"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yeZawWlEIQpe"
      },
      "source": [
        "import time, numpy as np\r\n",
        "n = 10**7\r\n",
        "data = np.random.randn(n)\r\n",
        "\r\n",
        "start = time.perf_counter()\r\n",
        "mean = sum(data)/len(data)                         #Python's sum\r\n",
        "seconds = time.perf_counter() - start\r\n",
        "\r\n",
        "print('That took {:.2f} seconds.'.format(seconds))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8rtxIEYCItW1"
      },
      "source": [
        "import time, numpy as np\r\n",
        "n = 10**7\r\n",
        "data = np.random.randn(n)\r\n",
        "\r\n",
        "start = time.perf_counter()\r\n",
        "mean = np.mean(data)                             #Numpy mean\r\n",
        "seconds = time.perf_counter() - start\r\n",
        "\r\n",
        "print('That took {:.2f} seconds.'.format(seconds))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5boMsFUqLver"
      },
      "source": [
        "import sys                 # Python Memory\r\n",
        "\r\n",
        "a = 3\r\n",
        "b = 3.123\r\n",
        "c = [a, b]\r\n",
        "d = []\r\n",
        "for obj in [a, b, c, d]:\r\n",
        "  print(obj, sys.getsizeof(obj))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nrlzgtvWRg2X"
      },
      "source": [
        "**Better Solution**\r\n",
        "\r\n",
        "(200 x 200) -> (50 x 50)\r\n",
        "\r\n",
        "200/50 = 4\r\n",
        "\r\n",
        "4 x 4 = 16\r\n",
        "\r\n",
        "Memory needed = 192/16 = 12GB "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4J-GkIJzSUN4"
      },
      "source": [
        "**Binapprox Algorithm**\r\n",
        "\r\n",
        "Load Image -> Bin each pixel value into a histogram at that pixel -> Generate histogram of pixel values at each pixel coordinate -> Sum counts in each histogram, starting from lowest bin -> When sum exceeds half the number of pixel values, stop -> Approximate median as central value of final bin added to sum"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5M6sPRt6VAVa"
      },
      "source": [
        "***Algorithm***\r\n",
        "\r\n",
        "The binapprox algorithm uses the method from the previous slide, but it saves even more time and space by only looking for the median within one standard deviation of the mean (see the link if you’d like to know why that works).\r\n",
        "\r\n",
        "The full algorithm for a set of N data points works as follows:\r\n",
        "\r\n",
        "    Calculate their mean and standard deviation, μ and σ;\r\n",
        "    Set the bounds: minval = μ−σ and maxval = μ+σ. Any value >= maxval is ignored;\r\n",
        "    Set the bin width: width = 2σ/B;\r\n",
        "    Make an ignore bin for counting value < minval;\r\n",
        "    Make B bins for counting values in minval and maxval, e.g. the first bin is minval <= value < minval + width;\r\n",
        "    Count the number of values that fall into each bin;\r\n",
        "    Sum these counts until total >= (N + 1)/2. Remember to start from the ignore bin;\r\n",
        "    Return the midpoint of the bin that exceeded (N + 1)/2.\r\n",
        "\r\n",
        "The midpoint of a bin is just the average of its min and max boundaries, i.e. the lower boundary + width/2.\r\n",
        "\r\n",
        "As soon as the relevant bin is updated the data point being binned can be removed from memory. So if you're finding the median of a bunch of FITS files you only need to have one loaded at any time. (The mean and standard deviation can both be calculated from running sums so that still applies to the first step).\r\n",
        "\r\n",
        "The downside of using binapprox is that you only get an answer accurate to σB by using B bins. Scientific data comes with its own uncertainties though, so as long as you keep B large enough this isn't necessarily a problem. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jp59FHPqTAIJ"
      },
      "source": [
        "import numpy as np\r\n",
        "\r\n",
        "def median_bins(values, B):\r\n",
        "  mean = np.mean(values)\r\n",
        "  std = np.std(values)\r\n",
        "    \r\n",
        "  # Initialise bins\r\n",
        "  left_bin = 0\r\n",
        "  bins = np.zeros(B)\r\n",
        "  bin_width = 2*std/B\r\n",
        "    \r\n",
        "  # Bin values\r\n",
        "  for value in values:\r\n",
        "    if value < mean - std:\r\n",
        "      left_bin += 1\r\n",
        "    elif value < mean + std:\r\n",
        "      bin = int((value - (mean - std))/bin_width)\r\n",
        "      bins[bin] += 1\r\n",
        "    # Ignore values above mean + std\r\n",
        "\r\n",
        "  return mean, std, left_bin, bins\r\n",
        "\r\n",
        "\r\n",
        "def median_approx(values, B):\r\n",
        "  # Call median_bins to calculate the mean, std,\r\n",
        "  # and bins for the input values\r\n",
        "  mean, std, left_bin, bins = median_bins(values, B)\r\n",
        "    \t\r\n",
        "  # Position of the middle element\r\n",
        "  N = len(values)\r\n",
        "  mid = (N + 1)/2\r\n",
        "\r\n",
        "  count = left_bin\r\n",
        "  for b, bincount in enumerate(bins):\r\n",
        "    count += bincount\r\n",
        "    if count >= mid:\r\n",
        "      # Stop when the cumulative count exceeds the midpoint\r\n",
        "      break\r\n",
        "\r\n",
        "  width = 2*std/B\r\n",
        "  median = mean - std + width*(b + 0.5)\r\n",
        "  return median"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UqCgLqJkIgI4"
      },
      "source": [
        "# Cross-matcher\r\n",
        "\r\n",
        "***Equatorial coordinates***\r\n",
        "\r\n",
        "The positions of stars, galaxies and other astronomical objects are usually recorded in either equatorial or Galactic coordinates.\r\n",
        "\r\n",
        "Equatorial coordinates are fixed relative to the celestial sphere, so the positions are independent of when or where the observations took place. They are defined relative to the celestial equator (which is in the same plane as the Earth's equator) and the ecliptic (the path the sun traces throughout the year).\r\n",
        "\r\n",
        "A point on the celestial sphere is given by two coordinates:\r\n",
        "\r\n",
        "    Right ascension: the angle from the vernal equinox to the point, going east along the celestial equator;\r\n",
        "    Declination: the angle from the celestial equator to the point, going north (negative values indicate going south).\r\n",
        "\r\n",
        "The vernal equinox is the intersection of the celestial equator and the ecliptic where the ecliptic rises above the celestial equator going further east. \r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "***Coordinates: Right Ascension***\r\n",
        "\r\n",
        "Right ascension is often given in hours-minutes-seconds (HMS) notation, because it was convenient to calculate when a star would appear over the horizon. A full circle in HMS notation is 24 hours, which means 1 hour in HMS notation is equal to 15 degrees.\r\n",
        "\r\n",
        "Each hour is split into 60 minutes and each minute into 60 seconds. \r\n",
        "\r\n",
        "\r\n",
        "***Coordinates : Declination***\r\n",
        "\r\n",
        " Declination is recorded in degrees-minutes-seconds (DMS) notation. A full circle is 360 degrees, each degree has 60 arcminutes and each arcminute has 60 arcseconds.\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TNAcyKeoR7Wu"
      },
      "source": [
        "print(15*(23 + 12/60 + 6/(60*60)))    # Converting HMS into degrees\r\n",
        "\r\n",
        "print(73 + 21/60 + 14.4/(60*60))      # Converting DMS into degrees\r\n",
        "\r\n",
        "print(-1*(5 + 31/60 + 12/(60*60)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FhCrvkIMiDDg"
      },
      "source": [
        "def hms2dec(hours, minutes, seconds):\r\n",
        "  return 15*(hours + minutes/60 + seconds/(60*60))\r\n",
        "\r\n",
        "def dms2dec(degrees, minutes, seconds):\r\n",
        "  if degrees<0:\r\n",
        "    sign = -1\r\n",
        "  elif degrees>0:\r\n",
        "    sign = 1\r\n",
        "  \r\n",
        "  return sign*(abs(degrees) + minutes/60 + seconds/(60*60))\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oxw7ZrgTh6ks"
      },
      "source": [
        "**Angular Distance**\r\n",
        "\r\n",
        " To crossmatch two catalogues we need to compare the angular distance between objects on the celestial sphere.\r\n",
        "\r\n",
        "People loosely call this a \"distance\", but technically its an angular distance: the projected angle between objects as seen from Earth.\r\n",
        "\r\n",
        "If we have an object on the celestial sphere with right ascension and declination (α1,δ1), then the angular distance to another object with coordinates (α2,δ2) is:\r\n",
        "\r\n",
        "d=2arcsin√sin2|δ1−δ2|2+cosδ1cosδ2sin2|α1−α2|2\r\n",
        "\r\n",
        "Angular distances have the same units as angles (degrees). There are other equations for calculating the angular distance but this one, called the haversine formula, is good at avoiding floating point errors when the two points are close together. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ffbIZPR6jGgN"
      },
      "source": [
        "import numpy as np\r\n",
        "\r\n",
        "def angular_dist(RA1, dec1, RA2, dec2):\r\n",
        "  # Convert to radians\r\n",
        "  r1 = np.radians(RA1)\r\n",
        "  d1 = np.radians(dec1)\r\n",
        "  r2 = np.radians(RA2)\r\n",
        "  d2 = np.radians(dec2)\r\n",
        "\r\n",
        "  a = np.sin(np.abs(d1 - d2)/2)**2\r\n",
        "  b = np.cos(d1)*np.cos(d2)*np.sin(np.abs(r1 - r2)/2)**2\r\n",
        "\r\n",
        "  angle = 2*np.arcsin(np.sqrt(a + b))\r\n",
        "    \r\n",
        "  # Convert back to degrees\r\n",
        "  return np.degrees(angle)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nEnF7EASm4c8"
      },
      "source": [
        "import_bss and import_super functions that import the AT20G BSS and SuperCOSMOS catalogues from the files bss.dat and super.csv. \r\n",
        "Each function should return a list of tuples containing the object's ID (an integer) and the coordinates in degrees. The object ID should be the row of the object in the catalogue, starting at 1. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bTLvCHL9nTT1"
      },
      "source": [
        "import numpy as np\r\n",
        "\r\n",
        "def hms2dec(hr, m, s):\r\n",
        "  dec = hr + m/60 + s/3600\r\n",
        "  return dec*15\r\n",
        "\r\n",
        "def dms2dec(d, m, s):\r\n",
        "  sign = d/abs(d)\r\n",
        "  dec = abs(d) + m/60 + s/3600\r\n",
        "  return sign*dec\r\n",
        "\r\n",
        "def import_bss():\r\n",
        "  res = []\r\n",
        "  data = np.loadtxt('bss.dat', usecols=range(1, 7))\r\n",
        "  for i, row in enumerate(data, 1):\r\n",
        "    res.append((i, hms2dec(row[0], row[1], row[2]), dms2dec(row[3], row[4], row[5])))\r\n",
        "  return res\r\n",
        "\r\n",
        "def import_super():\r\n",
        "  data = np.loadtxt('super.csv', delimiter=',', skiprows=1, usecols=(0, 1))\r\n",
        "  res = []\r\n",
        "  for i, row in enumerate(data, 1):\r\n",
        "    res.append((i, row[0], row[1]))\r\n",
        "  return res"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}